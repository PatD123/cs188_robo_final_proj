<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multimodal Human-Robot Interaction</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 2rem auto;
            padding: 0 1rem;
        }

        h1,
        h2,
        h3 {
            color: #2c3e50;
        }

        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 3px;
        }

        pre {
            background-color: #f4f4f4;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
    </style>
</head>

<body>
    <h1>Multimodal Human-Robot Interaction: Integrating Gesture and Speech Commands for Robotic Arm Control</h1>

    <h2>Abstract</h2>
    <p>This paper presents a multimodal interface for controlling robotic arms through the integration of hand gesture
        recognition and natural language speech commands. Our system combines MediaPipe for real-time hand gesture
        detection with RealtimeSTT for speech transcription and Llama3.2 for natural language command interpretation.
        The approach is evaluated in the RoboSuite simulation environment using a Franka Panda arm performing block
        stacking tasks. Results demonstrate successful multimodal integration while highlighting latency challenges in
        local LLM processing. The system achieves improved task completion through complementary gesture and speech
        modalities.</p>

    <h2>System Architecture</h2>
    <p>Our multimodal robotic control system consists of two input modules which collect and process spatial and audio
        signals, before sending control signals into the PID module, which controls the robot. The system runs on a
        multi-threaded architecture that enables real-time processing of both gesture and speech inputs simultaneously,
        one thread handling gesture recognition and another handling speech-to-action.</p>

    <h3>Gesture Recognition</h3>
    <p>We use Mediapipe to identify the thumb and index finger positions. If these two fingers are perpendicular to each
        other, whatever direction our index finger is pointing relative to the x-axis of the webcam frame, we will move
        in that direction.
    </p>
    <video width="848" height="464" controls>
        <source src="GESTURES.mp4" type="video/mp4">
    </video>

    <h3>3.3 Speech Recognition</h3>
    <p>The speech recognition and processing module is split into two steps. Firstly, the speech to text system utilizes
        the RealtimeSTT library and OpenAI whisper for continuous audio capture and transcription.</p>
    <p>Subsequently, the transcribed text is sent to a locally-hosted Llama3.2 language model accessed via the Ollama
        framework. We use the following prompt to convert the natural language into predefined structured control
        inputs:</p>
    <pre><code>We are controlling a robot arm using voice commands. The user can control if the gripper is open or closed, rotate the arm, move the arm forward or backward, and change the speed of the arm. Based on the user's input, return the action to be taken. Return only the action, no other text. The action must be one of the following: … Here is the user's input: {text}</code></pre>
    <p>Using the LLM we are able to convert unstructured natural language to structured control inputs that are mapped
        to control actions.</p>
    <video width="848" height="464" controls>
        <source src="NLP.mp4" type="video/mp4">
    </video>

    <h3>3.4 Robotic Control</h3>
    <p>At this stage, we have mapped either a speech command or a hand gesture to a certain movement direction. Here, we
        then use a PID controller to move our robot’s end effector to a specific location in that vector direction. For
        example, if our gestured direction was “UP,” it would be mapped to a normal vector [0, 0, 1] and we would move
        our end effector to a location 1 unit above our current location. This methodology is similarly applied to all
        further movement directions.</p>

    <h2>Full Demo Evaluation</h2>

    <video width="848" height="464" controls>
        <source src="FULL_DEMO.mp4" type="video/mp4">
    </video>

    <h2>5. Discussion and Reflections</h2>

    <h3>5.1 Technical Contributions and Insights</h3>
    <p>This research demonstrates the practical feasibility of integrating gesture recognition with local language model
        processing for robotic control applications. The successful combination of real-time visual input with natural
        language commands effectively leverages each modality's inherent strengths. The gesture recognition system
        provides immediate spatial control with minimal latency, while speech commands enable semantic task
        specification, parameter adjustments, and a wider action space that would be difficult to achieve through
        gestures alone.</p>

    <h3>5.2 Challenges and Limitations</h3>
    <p>The most significant challenge identified is the latency bottleneck associated with local language model
        processing. The 3.45-second average response time for speech commands significantly exceeds the threshold for
        perceived real-time interaction, potentially reducing user satisfaction and limiting practical applicability.
        This can be improved with more powerful computing and GPU hardware.</p>
    <p>Similarly, the accuracy of the natural language evaluation can be improved with better hardware. We were limited
        to the Llama3.2 model due to its smaller size and quicker token processing rate to attempt to improve the
        latency. With more powerful compute hardware, better models can be used with improved natural language
        processing ability.</p>
    <p>Gesture recognition robustness represents another important limitation. While MediaPipe provides reliable hand
        tracking under controlled conditions, the current implementation shows sensitivity to environmental factors
        including lighting variations and hand occlusion. Additionally, the lack of depth sensing removes several
        intuitive gestures and actions from being recognized by the system.</p>

    <h2>6. Conclusion</h2>
    <p>This research presents a successful integration of gesture recognition and speech processing for multimodal
        robotic arm control, demonstrating measurable improvements in task completion rates compared to single-modality
        approaches. The system architecture effectively combines MediaPipe's robust hand tracking capabilities with
        local language model processing through a threaded control framework that enables simultaneous processing of
        multiple input modalities.</p>
</body>

</html>